{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The basics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import Sklearn model libraries\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score, precision_score, recall_score, precision_recall_curve\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# Import Utility libraries\n",
    "import shap\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from tqdm import tqdm_notebook\n",
    "import warnings\n",
    "import ipyparallel as ipp\n",
    "import time\n",
    "from termcolor import colored\n",
    "import os\n",
    "# Attack Utilities\n",
    "# from create_models import create_models\n",
    "# from attack import attack\n",
    "# from attack_utils import baseline, load_data\n",
    "# from attack_data import attack_dataset, run_engines\n",
    "\n",
    "# Pre-run necessities\n",
    "warnings.filterwarnings('ignore')\n",
    "rnds = [60, 452, 774, 802, 961, 626, 726, 211, 375, 448, 883, 684, 724, 333, 64, 646, 116, 714, 483, 73, 562, 703, 276, 394, 190, 675, 314, 604, 297, 266, 456, 845, 822, 529, 420, 605, 935, 733, 167, 603, 401, 948, 241, 734, 550, 65, 429, 470, 633, 627, 223, 713, 958, 40, 200, 641, 357, 778, 781, 498, 202, 349, 983, 125, 548, 331, 206, 556, 356, 805, 246, 626, 358, 393, 307, 792, 777, 169, 595, 279, 719, 902, 124, 197, 983, 499, 368, 864, 896, 887, 879, 224, 220, 926, 565, 173, 919, 3, 908, 941]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class create_models():\n",
    "    \n",
    "    '''\n",
    "    Create datasets and models on the given dataset\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,x,y,target_size,n_models,random_state,want_tar_tuning,want_ref_tuning,tune_scoring,params,know):\n",
    "        \n",
    "        '''\n",
    "        Input should be of the size (NxD), Labels should be of the size (Nx1)\n",
    "        test_size_target,test_size_ref\n",
    "        '''\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.target_size = target_size\n",
    "        self.random_state = random_state\n",
    "        self.n_models = n_models\n",
    "        self.ref_model = []\n",
    "        self.tune_scoring = tune_scoring\n",
    "#         if know == 'disjoint':\n",
    "#             self.create_datasets_disjoint()\n",
    "#         elif know == 'shared':\n",
    "#             self.create_datasets_shared()\n",
    "        self.create_datasets_disjoint()\n",
    "        self.train_target(want_tar_tuning,params)\n",
    "        self.train_reference(want_ref_tuning,params)\n",
    "        tr_acc,te_acc = self.target_performance()\n",
    "        \n",
    "#         print(colored(\"Target Metrics\",'green'),end=\": \")\n",
    "#         print(\"Train Accuracy= {a}, Test Accuracy= {b}\".format(a=np.round(tr_acc,3),b=np.round(te_acc,3)))\n",
    "        \n",
    "    def set_parameters(self,model,params,x,y):\n",
    "        \n",
    "        grid_search = GridSearchCV(model,params,scoring=self.tune_scoring,n_jobs=-1)\n",
    "        grid_search.fit(x,y)\n",
    "        best_parameters = grid_search.best_params_\n",
    "        return best_parameters\n",
    "        \n",
    "    def create_datasets_disjoint(self):\n",
    "        \n",
    "        # Make target and reference pool distinct\n",
    "        x_ref,x_tar,y_ref,y_tar = train_test_split(self.x,self.y,test_size=self.target_size,random_state=self.random_state)\n",
    "        self.target_data_full = (x_tar,y_tar) # for Yeom et al approach\n",
    "        self.ref_data_full = (x_ref,y_ref) # for Yeom et al approach\n",
    "        \n",
    "        # Split the target data into train and test\n",
    "        self.x_tar_train,self.x_tar_test,self.y_tar_train,self.y_tar_test = train_test_split(x_tar,y_tar,test_size=0.5,random_state=self.random_state)\n",
    "        x_ref_train_,x_ref_test_,y_ref_train_,y_ref_test_ = [],[],[],[]\n",
    "        ref_rnds = [60, 452, 774, 802, 961, 626, 726, 211, 375, 448, 883, 684, 724, 333, 64, 646, 116, 714, 483, 73, 562, 703, 276, 394, 190, 675, 314, 604, 297, 266, 456, 845, 822, 529, 420, 605, 935, 733, 167, 603, 401, 948, 241, 734, 550, 65, 429, 470, 633, 627]\n",
    "        for i in range(self.n_models):\n",
    "            x_ref_train,x_ref_test,y_ref_train,y_ref_test = train_test_split(x_ref,y_ref,test_size=0.5,random_state=ref_rnds[i])\n",
    "            x_ref_train_.append(x_ref_train)\n",
    "            x_ref_test_.append(x_ref_test)\n",
    "            y_ref_train_.append(y_ref_train)\n",
    "            y_ref_test_.append(y_ref_test)\n",
    "            \n",
    "        self.x_ref_train,self.x_ref_test,self.y_ref_train,self.y_ref_test = x_ref_train_,x_ref_test_,y_ref_train_,y_ref_test_\n",
    "        \n",
    "        \n",
    "    def create_datasets_shared(self):\n",
    "        \n",
    "        # Make target and reference pool distinct\n",
    "        _,x_tar,_,y_tar = train_test_split(self.x,self.y,test_size=self.target_size,random_state=self.random_state)\n",
    "        _,x_ref,_,y_ref = train_test_split(self.x,self.y,test_size=self.target_size,random_state=self.random_state+1)\n",
    "        self.target_data_full = (x_tar,y_tar) # for Yeom et al approach\n",
    "        self.ref_data_full = (x_ref,y_ref) # for Yeom et al approach\n",
    "        \n",
    "        # Split the target data into train and test\n",
    "        self.x_tar_train,self.x_tar_test,self.y_tar_train,self.y_tar_test = train_test_split(x_tar,y_tar,test_size=0.5,random_state=self.random_state)\n",
    "        x_ref_train_,x_ref_test_,y_ref_train_,y_ref_test_ = [],[],[],[]\n",
    "        ref_rnds = [60, 452, 774, 802, 961, 626, 726, 211, 375, 448, 883, 684, 724, 333, 64, 646, 116, 714, 483, 73, 562, 703, 276, 394, 190, 675, 314, 604, 297, 266, 456, 845, 822, 529, 420, 605, 935, 733, 167, 603, 401, 948, 241, 734, 550, 65, 429, 470, 633, 627]\n",
    "        for i in range(self.n_models):\n",
    "            x_ref_train,x_ref_test,y_ref_train,y_ref_test = train_test_split(x_ref,y_ref,test_size=0.5,random_state=ref_rnds[i])\n",
    "            x_ref_train_.append(x_ref_train)\n",
    "            x_ref_test_.append(x_ref_test)\n",
    "            y_ref_train_.append(y_ref_train)\n",
    "            y_ref_test_.append(y_ref_test)\n",
    "            \n",
    "        self.x_ref_train,self.x_ref_test,self.y_ref_train,self.y_ref_test = x_ref_train_,x_ref_test_,y_ref_train_,y_ref_test_\n",
    "        \n",
    "    def train_target(self,want_tar_tuning,params):\n",
    "        \n",
    "        if want_tar_tuning==True:\n",
    "            self.target_model = RandomForestClassifier(random_state=self.random_state)\n",
    "            best_params = self.set_parameters(self.target_model,params,self.x_tar_train.iloc[:,:-1],self.y_tar_train.iloc[:,:-1])\n",
    "            self.target_model.set_params(n_estimators = best_params['n_estimators'])\n",
    "            self.target_model.set_params(max_depth = best_params['max_depth'])\n",
    "            \n",
    "#             print(colored(\"Best parameters - Target Model\",'red'),end=': ')\n",
    "#             print('n_estimators= {a}, max_depth= {b}'.format(a=best_params['n_estimators'],\n",
    "#                                                              b=best_params['max_depth']))\n",
    "            \n",
    "            self.target_model.fit(self.x_tar_train.iloc[:,:-1],self.y_tar_train.iloc[:,:-1])\n",
    "        elif want_tar_tuning==False:\n",
    "            self.target_model = RandomForestClassifier(random_state=self.random_state)\n",
    "            self.target_model.fit(self.x_tar_train.iloc[:,:-1],self.y_tar_train.iloc[:,:-1])\n",
    "        \n",
    "    def train_reference(self,want_ref_tuning,params):\n",
    "\n",
    "        for i in range(self.n_models):\n",
    "            if want_ref_tuning==True:\n",
    "                ref_model_ = RandomForestClassifier(random_state=self.random_state)\n",
    "                best_params = self.set_parameters(ref_model_,params,self.x_ref_train[i].iloc[:,:-1],self.y_ref_train[i].iloc[:,:-1])\n",
    "                ref_model_.set_params(n_estimators = best_params['n_estimators'])\n",
    "                ref_model_.set_params(max_depth = best_params['max_depth'])\n",
    "\n",
    "#                 print(colored(\"Best parameters - Reference Model\",'yellow'),end=': ')\n",
    "#                 print('n_estimators= {a}, max_depth= {b}'.format(a=best_params['n_estimators'],\n",
    "#                                                                  b=best_params['max_depth']))\n",
    "\n",
    "                ref_model_.fit(self.x_ref_train[i].iloc[:,:-1],self.y_ref_train[i].iloc[:,:-1])\n",
    "            elif want_ref_tuning==False:\n",
    "                ref_model_ = RandomForestClassifier(random_state=self.random_state)\n",
    "                ref_model_.fit(self.x_ref_train[i].iloc[:,:-1],self.y_ref_train[i].iloc[:,:-1])\n",
    "\n",
    "            self.ref_model.append(ref_model_)\n",
    "    \n",
    "    def target_performance(self):\n",
    "        return self.target_model.score(self.x_tar_train.iloc[:,:-1],self.y_tar_train.iloc[:,:-1]), self.target_model.score(self.x_tar_test.iloc[:,:-1],self.y_tar_test.iloc[:,:-1])\n",
    "    \n",
    "    def get_tar_data(self):\n",
    "        return self.x_tar_train,self.x_tar_test,self.y_tar_train,self.y_tar_test,self.target_data_full\n",
    "    \n",
    "    def get_ref_data(self):\n",
    "        return self.x_ref_train,self.x_ref_test,self.y_ref_train,self.y_ref_test,self.ref_data_full\n",
    "    \n",
    "    def get_models(self):\n",
    "        return self.ref_model, self.target_model\n",
    "    \n",
    "    def get_predprob(self):\n",
    "\n",
    "        tar_train_pred = self.target_model.predict_proba(self.x_tar_train.iloc[:,:-1])[:,0].reshape(self.x_tar_train.shape[0],1)\n",
    "        tar_test_pred = self.target_model.predict_proba(self.x_tar_test.iloc[:,:-1])[:,0].reshape(self.x_tar_test.shape[0],1)\n",
    "#         tar_train_ids = np.array(self.x_tar_train.iloc[:,-1]).reshape(self.x_tar_train.shape[0],1)\n",
    "#         tar_test_ids = np.array(self.x_tar_test.iloc[:,-1]).reshape(self.x_tar_test.shape[0],1)\n",
    "        \n",
    "#         pred_train_target_model = np.concatenate((tar_train_pred,tar_train_ids),axis=-1)\n",
    "#         pred_test_target_model = np.concatenate((tar_test_pred,tar_test_ids),axis=-1)\n",
    "#         pred_target_model = np.concatenate((pred_train_target_model,pred_test_target_model))\n",
    "        pred_target_model = np.concatenate((tar_train_pred,tar_test_pred))\n",
    "        pred_target = pred_target_model\n",
    "        \n",
    "        pred_ref_list = []\n",
    "        for i in range(self.n_models):\n",
    "            \n",
    "            ref_train_pred = self.ref_model[i].predict_proba(self.x_ref_train[i].iloc[:,:-1])[:,0].reshape(self.x_ref_train[i].shape[0],1)\n",
    "            ref_test_pred = self.ref_model[i].predict_proba(self.x_ref_test[i].iloc[:,:-1])[:,0].reshape(self.x_ref_test[i].shape[0],1)\n",
    "#             ref_train_ids = np.array(self.x_ref_train[i].iloc[:,-1]).reshape(self.x_ref_train[i].shape[0],1)\n",
    "#             ref_test_ids = np.array(self.x_ref_test[i].iloc[:,-1]).reshape(self.x_ref_test[i].shape[0],1)\n",
    "            \n",
    "#             pred_train_ref_model = np.concatenate((ref_train_pred,ref_train_ids),axis=-1)\n",
    "#             pred_test_ref_model = np.concatenate((ref_test_pred,ref_test_ids),axis=-1)\n",
    "            pred_ref_model = np.concatenate((ref_train_pred,ref_test_pred))\n",
    "            pred_ref_list.append(pred_ref_model)\n",
    "            \n",
    "        return pred_target, pred_ref_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_SHAP_values(model):\n",
    "    import shap\n",
    "    for i in range(points.shape[0]):\n",
    "        explainer = shap.TreeExplainer(model)\n",
    "        shap_values = explainer.shap_values(points[i],check_additivity=False)\n",
    "        shap_val.append(shap_values[0]) \n",
    "\n",
    "def get_dp_impurity(model):\n",
    "    import numpy as np\n",
    "    sample_ids =np.arange(0,points.shape[0],1)\n",
    "    for j in range(len(sample_ids)):\n",
    "        sp = np.zeros(points.shape[1]+1)\n",
    "        for i in range(len(model.estimators_)):\n",
    "            node_indicator = model.estimators_[i].decision_path(points[j,:].reshape(1,points.shape[1]))\n",
    "            impurity = model.estimators_[i].tree_.impurity\n",
    "            features = model.estimators_[i].tree_.feature\n",
    "            node_index = node_indicator.indices[node_indicator.indptr[0]:node_indicator.indptr[1]]\n",
    "            for node in node_index.tolist():\n",
    "                f = features[node]\n",
    "                if f == -2:\n",
    "                    f=-1\n",
    "                sp[f] += impurity[node]\n",
    "        shap_val.append(sp/len(model.estimators_))\n",
    "        \n",
    "def get_dp_nodesamples(model):\n",
    "    import numpy as np\n",
    "    sample_ids =np.arange(0,points.shape[0],1)\n",
    "    for j in range(len(sample_ids)):\n",
    "        sp = np.zeros(points.shape[1]+1)\n",
    "        for i in range(len(model.estimators_)):\n",
    "            node_indicator = model.estimators_[i].decision_path(points[j,:].reshape(1,points.shape[1]))\n",
    "            weighted_n_node_samples = model.estimators_[i].tree_.weighted_n_node_samples\n",
    "            features = model.estimators_[i].tree_.feature\n",
    "            node_index = node_indicator.indices[node_indicator.indptr[0]:node_indicator.indptr[1]]\n",
    "            for node in node_index.tolist():\n",
    "                f = features[node]\n",
    "                if f == -2:\n",
    "                    f=-1\n",
    "                sp[f] += weighted_n_node_samples[node]\n",
    "        shap_val.append(sp/len(model.estimators_))\n",
    "        \n",
    "def get_dp_importance(model):\n",
    "    import numpy as np\n",
    "    sample_ids =np.arange(0,points.shape[0],1)\n",
    "    for j in range(len(sample_ids)):\n",
    "        sp = np.zeros(points.shape[1])\n",
    "        for i in range(len(model.estimators_)):\n",
    "            node_indicator = model.estimators_[i].decision_path(points[j,:].reshape(1,points.shape[1]))\n",
    "            importance = model.estimators_[i].feature_importances_\n",
    "            features = model.estimators_[i].tree_.feature\n",
    "            node_index = node_indicator.indices[node_indicator.indptr[0]:node_indicator.indptr[1]]\n",
    "            for node in node_index.tolist():\n",
    "                f = features[node]\n",
    "                if f == -2:\n",
    "                    continue\n",
    "                sp[f] += importance[f]\n",
    "        shap_val.append(sp/len(model.estimators_))\n",
    "        \n",
    "def get_dp_threshold(model):\n",
    "    import numpy as np\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    sample_ids =np.arange(0,points.shape[0],1)\n",
    "    for j in range(len(sample_ids)):\n",
    "        sp = np.zeros(points.shape[1]+1)\n",
    "        for i in range(len(model.estimators_)):\n",
    "            node_indicator = model.estimators_[i].decision_path(points[j,:].reshape(1,points.shape[1]))\n",
    "            threshold = MinMaxScaler().fit_transform(model.estimators_[i].tree_.threshold.reshape(-1,1))\n",
    "            features = model.estimators_[i].tree_.feature\n",
    "            node_index = node_indicator.indices[node_indicator.indptr[0]:node_indicator.indptr[1]]\n",
    "            for node in node_index.tolist():\n",
    "                f = features[node]\n",
    "                if f == -2:\n",
    "                    f=-1\n",
    "                sp[f] += threshold[node]\n",
    "        shap_val.append(sp)\n",
    "\n",
    "\n",
    "class attack_dataset():\n",
    "    \n",
    "    def __init__(self,rc,tar_model,ref_model,tar_data,ref_data,pred_target,pred_ref,influence):\n",
    "        \n",
    "        if influence == 'SHAP':\n",
    "            self.method = get_SHAP_values\n",
    "        elif influence == 'DP_impurity':\n",
    "            self.method = get_dp_impurity\n",
    "        elif influence == 'DP_threshold':\n",
    "            self.method = get_dp_threshold\n",
    "        elif influence == 'DP_importance':\n",
    "            self.method = get_dp_importance\n",
    "        elif influence == 'DP_nodesamples':\n",
    "            self.method = get_dp_nodesamples\n",
    "        \n",
    "        res_tar = self.parallelism(rc,tar_data,tar_model)\n",
    "        res_ref = self.parallelism(rc,ref_data,ref_model)\n",
    "        self.tsp,self.rsp = self.retrieve((res_tar,res_ref),pred_target,pred_ref)\n",
    "\n",
    "    def parallelism(self,rc,data_parallel,model):\n",
    "        view = rc[:]\n",
    "        view.scatter('points',data_parallel)\n",
    "        view.scatter('shap_val',[])\n",
    "        view.block=True\n",
    "        view.apply(self.method,model)\n",
    "        res = view.gather('shap_val')\n",
    "        return res\n",
    "\n",
    "    def retrieve(self,results_read,pred_target,pred_ref):\n",
    "\n",
    "        shaps_tar, shaps_ref = results_read[0], results_read[1]\n",
    "        shaps_tar = np.array(shaps_tar)\n",
    "        shaps_ref = np.array(shaps_ref)\n",
    "        tar_shap_prob = np.concatenate((shaps_tar,pred_target),axis=-1)\n",
    "        ref_shap_prob = np.concatenate((shaps_ref,pred_ref),axis=-1)\n",
    "\n",
    "        return tar_shap_prob,ref_shap_prob\n",
    "    \n",
    "    def get_data(self):\n",
    "        return self.tsp,self.rsp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class attack():\n",
    "    \n",
    "    def __init__(self,r,want_tune,tune_scoring):\n",
    "        self.rs = r\n",
    "        self.want_tune=want_tune\n",
    "        self.tune_scoring=tune_scoring\n",
    "    \n",
    "    def set_parameters(self,model,x,y):\n",
    "        params = {'n_estimators':[20,50,100,150,200],'max_depth':[3,4,5,6,7]}\n",
    "        grid_search = GridSearchCV(model,params,scoring=self.tune_scoring,n_jobs=-1)\n",
    "        grid_search.fit(x,y)\n",
    "        best_parameters = grid_search.best_params_\n",
    "        return best_parameters\n",
    "\n",
    "    def fit(self,x,y):\n",
    "        self.attack_model = RandomForestClassifier(random_state=self.rs)\n",
    "        if self.want_tune:\n",
    "            best_params = self.set_parameters(self.attack_model,x,y)\n",
    "            self.attack_model.set_params(n_estimators = best_params['n_estimators'])\n",
    "            self.attack_model.set_params(max_depth = best_params['max_depth'])\n",
    "#             print(colored(\"Best parameters - Attack model\",'blue'),end=\": \")\n",
    "#             print('n_estimators= {a}, max_depth= {b}\\n'.format(a=best_params['n_estimators'],b=best_params['max_depth']))\n",
    "\n",
    "        self.attack_model.fit(x,y)\n",
    "    \n",
    "    def predict(self,x):\n",
    "\n",
    "        return self.attack_model.predict(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline(tar_model,target_data,tar_lab,true_lab):\n",
    "    preds = tar_model.predict(target_data)\n",
    "    mem_preds = np.ones(preds.shape[0])\n",
    "    mem_preds[preds!=true_lab]=0\n",
    "    acc_sc = accuracy_score(tar_lab,mem_preds)\n",
    "    pr_sc = precision_score(tar_lab,mem_preds)\n",
    "    rec_sc = recall_score(tar_lab,mem_preds)\n",
    "    f1_sc =f1_score(tar_lab,mem_preds)\n",
    "\n",
    "    return acc_sc,pr_sc,rec_sc,f1_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "DATASET LOAD\n",
    "'''\n",
    "def load_data(name):\n",
    "    \n",
    "    if name == 'adult_income':\n",
    "\n",
    "        filepath_text = 'adult-all.txt'\n",
    "        header_name = ['age','workclass','fnlwgt','education','education-num','marital-status','occupation','relationship','race','sex','capital-gain','capital-loss','hours-per-week','native-country','income']\n",
    "        data = pd.read_table(filepath_text,sep=',',names=header_name)\n",
    "\n",
    "        # Encode features\n",
    "        categorical_feature_mask = data.dtypes==object\n",
    "        category_col = data.columns[categorical_feature_mask].tolist()\n",
    "        for col in category_col:\n",
    "            b, c = np.unique(data[col], return_inverse=True) \n",
    "            data[col] = c\n",
    "        data = data.iloc[:20000,:]\n",
    "        mm = MinMaxScaler()\n",
    "        data_trans = mm.fit_transform(data)\n",
    "\n",
    "        data_trans = pd.DataFrame(data_trans,columns=header_name)\n",
    "        data_trans['id'] = np.arange(0,data_trans.shape[0])\n",
    "        y = data_trans[['income','id']]\n",
    "        x = data_trans.drop(['income'],axis=1)\n",
    "        \n",
    "    elif name == 'bcw':\n",
    "\n",
    "        # Read data\n",
    "        filepath = '~/bcw/wdbc.data'\n",
    "        column_name = ['id', 'diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean',\n",
    "               'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n",
    "               'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n",
    "               'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n",
    "               'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n",
    "               'fractal_dimension_se', 'radius_worst', 'texture_worst',\n",
    "               'perimeter_worst', 'area_worst', 'smoothness_worst',\n",
    "               'compactness_worst', 'concavity_worst', 'concave points_worst',\n",
    "               'symmetry_worst', 'fractal_dimension_worst', 'Unnamed: 32']\n",
    "        data = pd.read_table(filepath, sep=',', names=column_name)\n",
    "        data_id = data['id']\n",
    "        # Create x and y for ML algorithm -- drop unnecesary features (id and last column)\n",
    "        y = data[['diagnosis','id']]\n",
    "        x = data.drop(['id','diagnosis','Unnamed: 32'],axis=1)\n",
    "        x['id'] = data_id\n",
    "        \n",
    "    elif name == 'german_credit':\n",
    "\n",
    "        # Read data\n",
    "        filepath = '~/german_credit/german.data'\n",
    "        column_name = ['A'+str(i) for i in range(1,21)]+['risk']\n",
    "        data = pd.read_table(filepath,sep=' ',names=column_name)\n",
    "        categorical_feature_mask = data.dtypes==object\n",
    "        category_col = data.columns[categorical_feature_mask].tolist()\n",
    "        for col in category_col:\n",
    "            b, c = np.unique(data[col], return_inverse=True) \n",
    "            data[col] = c\n",
    "\n",
    "        data['id'] = np.arange(0,data.shape[0])\n",
    "        y = data[['risk','id']]\n",
    "        x = data.drop(['risk'],axis=1)\n",
    "        \n",
    "    elif name == 'pima_diabetes':\n",
    "\n",
    "        # Read data\n",
    "        filepath = '~/pima_diabetes/diabetes.csv'\n",
    "        data = pd.read_csv(filepath)\n",
    "\n",
    "        data['id'] = np.arange(0,data.shape[0])\n",
    "        y = data[['Outcome','id']]\n",
    "        x = data.drop(['Outcome'],axis=1)\n",
    "        \n",
    "    elif name == 'hepatitis':\n",
    "\n",
    "        filepath = '~/hepatitis/hepatitis_clean.csv'\n",
    "        column_name=['Class','AGE','SEX','STEROID','ANTIVIRALS',\n",
    "                     'FATIGUE','MALAISE','ANOREXIA','LIVER BIG',\n",
    "                     'LIVER FIRM','SPLEEN PALPABLE','SPIDERS','ASCITES',\n",
    "                     'VARICES','BILIRUBIN','ALK PHOSPHATE','SGOT','ALBUMIN',\n",
    "                     'PROTIME','HISTOLOGY']\n",
    "\n",
    "        data = pd.read_csv(filepath)\n",
    "        data['id'] = np.arange(0,data.shape[0])\n",
    "        y = data[['HISTOLOGY','id']]\n",
    "        x = data.drop(['HISTOLOGY'],axis=1)\n",
    "        \n",
    "    elif name == 'heart_disease':\n",
    "\n",
    "        filepath = '~/heart_disease/processed.cleveland.data'\n",
    "        column_name=['age','sex','cp','trestbps','chol','fbs','restecg',\n",
    "                    'thalach','exang','oldpeak','slope','ca','thal','num']\n",
    "        data = pd.read_table(filepath,sep=',',names=column_name)\n",
    "\n",
    "        # 'ca' and 'thal' have '?' in values, hence clean\n",
    "        data['thal'].replace('?',data['thal'].mode().values[0],inplace=True)\n",
    "        data['ca'].replace('?',data['ca'].mode().values[0],inplace=True)\n",
    "\n",
    "        # 'ca' and 'thal' are of type 'object' because the value type is string, hence convert to float\n",
    "        for col in ['thal','ca']:\n",
    "            val = data[col]\n",
    "            val_n = [float(v) for v in val]\n",
    "            data[col] = pd.Series(val_n)\n",
    "\n",
    "        data['id'] = np.arange(0,data.shape[0])\n",
    "        y = data[['num','id']]\n",
    "        x = data.drop(['num'],axis=1)\n",
    "        \n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_data\n",
    "\n",
    "def write_into_files(to_store,run,dataset_name,influence_type,ref_tuning):\n",
    "    print(\"\\n######### Writing into file #########\")\n",
    "    if ref_tuning:\n",
    "        parent_dir = '/home/amanmoha/influence_attack/results/actual/RF/modules_ref_tuning/'\n",
    "    else:\n",
    "        parent_dir = '/home/amanmoha/influence_attack/results/actual/RF/modules/'\n",
    "    while True:\n",
    "        try: \n",
    "            directory = 'run'+run+'/'+dataset_name+'/'+influence_type\n",
    "            path = os.path.join(parent_dir,directory)\n",
    "            os.mkdir(path)\n",
    "            break\n",
    "        except FileNotFoundError:\n",
    "            try: \n",
    "                directory = 'run'+run+'/'+dataset_name\n",
    "                path = os.path.join(parent_dir,directory)\n",
    "                os.mkdir(path)\n",
    "                continue\n",
    "            except FileNotFoundError:\n",
    "                directory = 'run'+run\n",
    "                path = os.path.join(parent_dir,directory)\n",
    "                os.mkdir(path)\n",
    "                continue\n",
    "\n",
    "    record_file = 'record.txt'\n",
    "    with open(parent_dir+record_file, 'a') as f:\n",
    "        f.write(\"\\n\\nRun-%s\\n\"%run)\n",
    "        f.write('n_states=%d\\n'%n_states)\n",
    "        f.write('n_models=%d'%n_models)\n",
    "    f.close()\n",
    "\n",
    "    with open(path+'/data.txt','wb') as f:\n",
    "        pickle.dump(to_store, f)\n",
    "    f.close()\n",
    "\n",
    "    with open(path+'/metrics.txt','w') as f:\n",
    "        f.write(\"Target train accuracy:\\t {a}\\nTarget test accuracy:\\t {b}\\nAttack accuracy:\\t {c}\\nAttack precision:\\t {d}\\nAttack recall:\\t\\t {e}\\nAttack f1:\\t\\t {f}\\nBaseline accuracy:\\t {g}\\nBaseline precision:\\t {h}\\nBaseline recall:\\t {i}\\nBaseline f1:\\t\\t {j}\\n\".format(a=tr_ac/n_states,b=te_ac/n_states,c=ac/n_states,d=ps/n_states,e=rs/n_states,f=fs/n_states,g=ac_b/n_states,h=ps_b/n_states,i=rs_b/n_states,j=fs_b/n_states))\n",
    "    f.close()\n",
    "    print(\"\\nWrite complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2175c291809e40b19ec962b2a2b5d77a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Datasets', max=1, style=ProgressStyle(description_width='init…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06da5e821926467ebc7a4abfeabc590b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Influences:', max=3, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc3272a09d9544dbaaf7747e349c87d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Tunings', max=1, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m###################################################\u001b[0m\n",
      "\u001b[34mDataset used: \u001b[0m adult_income\n",
      "\u001b[34mNumber of samples: \u001b[0m 20000\n",
      "\u001b[34mNumber of features: \u001b[0m 15\n",
      "\u001b[34mNumber of models: \u001b[0m 10\n",
      "\u001b[34mNumber of seeds: \u001b[0m 15\n",
      "\u001b[34mNumber of parallel engines: \u001b[0m 20\n",
      "\u001b[34mKnowledge: \u001b[0m disjoint\n",
      "\u001b[34mInfluence type: \u001b[0m SHAP\n",
      "\u001b[34mReference Tuning: \u001b[0m True\n",
      "\u001b[34m###################################################\n",
      "\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7baaec0fe4e54854bccee21bb080f0a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Different random states', max=15, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RANDOM STATE:  60\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e7b9ff7e12f48399291e6a35c5b85b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Generating Data', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RANDOM STATE:  452\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b59c8b89040a407fbee0a2a35a5c8704",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Generating Data', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RANDOM STATE:  774\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf64fc5bd9564d27b2abbec4fb14d392",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Generating Data', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RANDOM STATE:  802\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8361f5a7c25e4b99a6de8d5c82bf832c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Generating Data', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RANDOM STATE:  961\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c96f1ed83d594217960f2969c3ed0983",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Generating Data', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RANDOM STATE:  626\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e01bd8ee98bc4a07bdb830179fd1189f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Generating Data', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RANDOM STATE:  726\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70a59cbf1871416cbe0546a5979b7638",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Generating Data', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RANDOM STATE:  211\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58a57b81b90347deb99a36cd4c0e54a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Generating Data', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RANDOM STATE:  375\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cb5b1e96e6c489ebc5491050d330227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Generating Data', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RANDOM STATE:  448\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d58c94de4593420a961ee51d2bc3dc9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Generating Data', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RANDOM STATE:  883\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98e6cda06adf4a46b0950b3d637222e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Generating Data', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RANDOM STATE:  684\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ee5375adbd346a9b776ced55b46569e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Generating Data', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RANDOM STATE:  724\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "635c0049f6c440d3a990e0f72b3785b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Generating Data', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RANDOM STATE:  333\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2aa1e3e046c465e9f5bf37a92b1eeeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Generating Data', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RANDOM STATE:  64\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "356e5a01af8440a08f150002a255a75e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Generating Data', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m################# RESULTS #################\n",
      "\u001b[0m\n",
      "Target train accuracy:\t 0.86788\n",
      "Target test accuracy:\t 0.8513733333333334\n",
      "Attack accuracy:\t 0.5253599999999999\n",
      "Attack precision:\t 0.5288591549191487\n",
      "Attack recall:\t\t 0.4672666666666666\n",
      "Attack f1:\t\t 0.4909204383880156\n",
      "Baseline accuracy:\t 0.5082533333333333\n",
      "Baseline precision:\t 0.5047990041247533\n",
      "Baseline recall:\t 0.86788\n",
      "Baseline f1:\t\t 0.6383180246652068\n",
      "\n",
      "\n",
      "######### Writing into file #########\n",
      "\n",
      "Write complete\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a38643266934a279588ac37eee9a4ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Tunings', max=1, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m###################################################\u001b[0m\n",
      "\u001b[34mDataset used: \u001b[0m adult_income\n",
      "\u001b[34mNumber of samples: \u001b[0m 20000\n",
      "\u001b[34mNumber of features: \u001b[0m 15\n",
      "\u001b[34mNumber of models: \u001b[0m 10\n",
      "\u001b[34mNumber of seeds: \u001b[0m 15\n",
      "\u001b[34mNumber of parallel engines: \u001b[0m 20\n",
      "\u001b[34mKnowledge: \u001b[0m disjoint\n",
      "\u001b[34mInfluence type: \u001b[0m DP_impurity\n",
      "\u001b[34mReference Tuning: \u001b[0m True\n",
      "\u001b[34m###################################################\n",
      "\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2736912e3e094dbaada4c3a4594c49de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Different random states', max=15, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RANDOM STATE:  60\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d809e67fd074dfda85343f076ef4fa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Generating Data', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RANDOM STATE:  452\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bdc96c9246740bd9f0e8844cd7c6317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Generating Data', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RANDOM STATE:  774\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "941a9f9e24524b4c84667f0eae94e51b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Generating Data', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RANDOM STATE:  802\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c63d9344f3704056adb2c640d58b7cca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Generating Data', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RANDOM STATE:  961\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eabb5919c56e44da85dbbb7c680e9ff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Generating Data', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RANDOM STATE:  626\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc0b0a28b768445abd31a665594cb3de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Generating Data', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RANDOM STATE:  726\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d705aebd7b03484fa1e225e0c7026176",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Generating Data', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RANDOM STATE:  211\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "301943b022e04e1e907e1c29bd30e831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Generating Data', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RANDOM STATE:  375\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fa5c029cd19453aa04dea1724abc9e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Generating Data', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RANDOM STATE:  448\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "799962a892c643c4b4c14f9f5903aa11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Generating Data', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RANDOM STATE:  883\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec85c24c67cb4e6db9c40dc8114d2da3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Generating Data', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RANDOM STATE:  684\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36655a6225fa42f6b4e99c380ad8a36c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Generating Data', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RANDOM STATE:  724\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aa66a046d344776914f690ca203329f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Generating Data', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RANDOM STATE:  333\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ec80f3387b24ebbb24e441d7d6e4e75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Generating Data', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RANDOM STATE:  64\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e38c9cf8b0454803b85bb0cd91f0491e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Generating Data', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m################# RESULTS #################\n",
      "\u001b[0m\n",
      "Target train accuracy:\t 0.86788\n",
      "Target test accuracy:\t 0.8513733333333334\n",
      "Attack accuracy:\t 0.5150266666666666\n",
      "Attack precision:\t 0.5166224282695455\n",
      "Attack recall:\t\t 0.44906666666666667\n",
      "Attack f1:\t\t 0.4710691624990523\n",
      "Baseline accuracy:\t 0.5082533333333333\n",
      "Baseline precision:\t 0.5047990041247533\n",
      "Baseline recall:\t 0.86788\n",
      "Baseline f1:\t\t 0.6383180246652068\n",
      "\n",
      "\n",
      "######### Writing into file #########\n",
      "\n",
      "Write complete\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baf89661bec2461ba690e733ce30e51d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Tunings', max=1, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m###################################################\u001b[0m\n",
      "\u001b[34mDataset used: \u001b[0m adult_income\n",
      "\u001b[34mNumber of samples: \u001b[0m 20000\n",
      "\u001b[34mNumber of features: \u001b[0m 15\n",
      "\u001b[34mNumber of models: \u001b[0m 10\n",
      "\u001b[34mNumber of seeds: \u001b[0m 15\n",
      "\u001b[34mNumber of parallel engines: \u001b[0m 20\n",
      "\u001b[34mKnowledge: \u001b[0m disjoint\n",
      "\u001b[34mInfluence type: \u001b[0m DP_importance\n",
      "\u001b[34mReference Tuning: \u001b[0m True\n",
      "\u001b[34m###################################################\n",
      "\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1cbe60ac15e412ca63b7537710d42fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Different random states', max=15, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RANDOM STATE:  60\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a3c890d75e04b218d6c4e7f5868fba8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Generating Data', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RANDOM STATE:  452\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4fd0446fdcf498b9c93ab22168d126e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Generating Data', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RANDOM STATE:  774\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5060f8527f04118b4c44a46b953647d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Generating Data', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RANDOM STATE:  802\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0177a65f193c4b638e29786ebd1a7109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Generating Data', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RANDOM STATE:  961\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "168d45f9ede9442ab7a91dbb7034c0c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Generating Data', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RANDOM STATE:  626\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da33e4f9b2f84605a11afa64ba601b1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Generating Data', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RANDOM STATE:  726\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cffa3cb5a5b34812ba9dde96ee3f9bd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Generating Data', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RANDOM STATE:  211\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e46480ab6ec24b9484c68ffb2879057b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Generating Data', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RANDOM STATE:  375\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5d13f27714c4db9948d5ef8456db2e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Generating Data', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RANDOM STATE:  448\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48217c02ed2642159d635a76a993b56a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Generating Data', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RANDOM STATE:  883\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b2f1dff706d4f5381c0ffaa08ac4ecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Generating Data', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RANDOM STATE:  684\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a64706724ca245cfb1bef8352183ad78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Generating Data', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RANDOM STATE:  724\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85acf24b4e944726bdba743607b13661",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Generating Data', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RANDOM STATE:  333\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2c40c5174e4458494ed86c87d0053c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Generating Data', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RANDOM STATE:  64\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd8a36fdefec4fccad97d397aaf4b29f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Generating Data', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m################# RESULTS #################\n",
      "\u001b[0m\n",
      "Target train accuracy:\t 0.86788\n",
      "Target test accuracy:\t 0.8513733333333334\n",
      "Attack accuracy:\t 0.5075866666666667\n",
      "Attack precision:\t 0.5119725820818499\n",
      "Attack recall:\t\t 0.4008133333333333\n",
      "Attack f1:\t\t 0.4318623386745673\n",
      "Baseline accuracy:\t 0.5082533333333333\n",
      "Baseline precision:\t 0.5047990041247533\n",
      "Baseline recall:\t 0.86788\n",
      "Baseline f1:\t\t 0.6383180246652068\n",
      "\n",
      "\n",
      "######### Writing into file #########\n",
      "\n",
      "Write complete\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###################\n",
    "# DRIVER\n",
    "###################\n",
    "'''\n",
    "DATASET NAMES: adult_income, bcw, german_credit, pima_diabetes, hepatitis, heart_disease\n",
    "KNOWLEGDE: disjoint, shared\n",
    "INFLUENCE TYPE: SHAP, DP_impurity, DP_threshold, DP_importance, DP_nodesamples\n",
    "DOUBLE INFLUENCE: False, True\n",
    "'''\n",
    "rc = ipp.Client()\n",
    "## INITIALIZATIONS\n",
    "dataset_names = ['adult_income']\n",
    "for dataset_name in tqdm_notebook(dataset_names,desc=\"Datasets\"):\n",
    "    x,y = load_data(dataset_name)\n",
    "#     ac,ps,rs,fs,tr_ac,te_ac=0,0,0,0,0,0\n",
    "#     ac_b,ps_b,rs_b,fs_b=0,0,0,0\n",
    "\n",
    "    ## CONTROL PARAMS\n",
    "    if dataset_name == 'hepatitis' or dataset_name == 'bcw' or dataset_name == 'german_credit':\n",
    "        tuning_list = {'n_estimators':[20,50,100],'max_depth':[3,4,5,6]}\n",
    "    elif dataset_name == 'pima_diabetes' or dataset_name == 'heart_disease':\n",
    "        tuning_list = {'n_estimators':[20],'max_depth':[3]}\n",
    "    elif dataset_name == 'adult_income':\n",
    "        tuning_list = {'n_estimators':[50,100,200],'max_depth':[4,5,6,7,8]}\n",
    "\n",
    "    n_states = 15\n",
    "#     n_models = 20\n",
    "    num_models = [10]\n",
    "    for n_models in num_models:\n",
    "        if n_states == 30 and n_models == 1:\n",
    "            run = '2'\n",
    "        elif n_states == 30 and n_models == 20:\n",
    "            run = '1'\n",
    "        elif n_states == 30 and n_models == 5:\n",
    "            run = '3'\n",
    "        elif n_states == 30 and n_models == 10:\n",
    "            run = '4'\n",
    "        elif n_states == 15 and n_models == 10:\n",
    "            run = '5'\n",
    "\n",
    "        # Yeom = False\n",
    "        knowledge = 'disjoint'\n",
    "        influence_types = ['SHAP','DP_impurity','DP_importance']\n",
    "\n",
    "        for influence_type in tqdm_notebook(influence_types,desc=\"Influences:\"):\n",
    "\n",
    "\n",
    "            # double_influence = False\n",
    "            # influence_type_second = 'DP_importance'        # if double_influence is true\n",
    "            target_params = {'target_size':0.5,\n",
    "                            'tuning':True,\n",
    "                            'scoring':'accuracy',\n",
    "                            'tune_params':tuning_list}\n",
    "            attack_params = {'tuning':True,\n",
    "                            'scoring':'f1'}\n",
    "            ref_tunings = [True]\n",
    "            for ref_tuning in tqdm_notebook(ref_tunings,desc=\"Tunings\"):\n",
    "\n",
    "                ac,ps,rs,fs,tr_ac,te_ac=0,0,0,0,0,0\n",
    "                ac_b,ps_b,rs_b,fs_b=0,0,0,0\n",
    "\n",
    "                #################################\n",
    "                # print information to screen\n",
    "                #################################\n",
    "                print(colored(\"###################################################\",'blue'))\n",
    "                print(colored(\"Dataset used: \",'blue'), dataset_name)\n",
    "                print(colored(\"Number of samples: \",'blue'), x.shape[0])\n",
    "                print(colored(\"Number of features: \",'blue'), x.shape[1])\n",
    "                print(colored(\"Number of models: \",'blue'),n_models)\n",
    "                print(colored(\"Number of seeds: \",'blue'),n_states)\n",
    "                print(colored(\"Number of parallel engines: \",'blue'),len(rc))\n",
    "                print(colored(\"Knowledge: \",'blue'), knowledge)\n",
    "                print(colored(\"Influence type: \",'blue'),influence_type)\n",
    "                print(colored(\"Reference Tuning: \",'blue'),ref_tuning)\n",
    "                # if double_influence:\n",
    "                #     print(colored(\"Double influence: \",'blue'),double_influence)\n",
    "                #     print(colored(\"Influence type second: \",'blue'),influence_type_second)\n",
    "                print(colored(\"###################################################\\n\",'blue'))\n",
    "\n",
    "                tar_data_all_states = []\n",
    "                ref_data_all_states = []\n",
    "                tar_lab_all_states = []\n",
    "                ref_lab_all_states = []\n",
    "                true_labels_all_states = []\n",
    "                ##### Creating all the models\n",
    "                for r in tqdm_notebook(rnds[:n_states],desc=\"Different random states\"):\n",
    "                    print(\"\\nRANDOM STATE: \",r)\n",
    "\n",
    "                    ## CREATE MODELS\n",
    "                    all_models = create_models(x,y,target_size=target_params['target_size'],n_models=n_models,random_state=r,\n",
    "                                               want_tar_tuning=target_params['tuning'],want_ref_tuning=ref_tuning,\n",
    "                                               tune_scoring=target_params['scoring'], params=target_params['tune_params'],\n",
    "                                               know=knowledge)\n",
    "\n",
    "                    ## GET MODELS' DATA\n",
    "                    x_tar_train,x_tar_test,y_tar_train,y_tar_test,target_data_full = all_models.get_tar_data()\n",
    "                    x_ref_train,x_ref_test,y_ref_train,y_ref_test,ref_data_full = all_models.get_ref_data()\n",
    "                    ref_model,target_model = all_models.get_models()\n",
    "                    pred_target,pred_ref = all_models.get_predprob()\n",
    "                    sv1,sv2 = all_models.target_performance()\n",
    "                    tr_ac+=sv1\n",
    "                    te_ac+=sv2\n",
    "\n",
    "                #     ## OVERLAP PERCENTAGE\n",
    "                #     if knowledge == 'shared':\n",
    "\n",
    "                #         x_tar_ids = target_data_full[0].iloc[:,-1]\n",
    "                #         x_ref_ids = ref_data_full[0].iloc[:,-1]\n",
    "                #         common_elements = np.intersect1d(x_tar_ids,x_ref_ids,assume_unique=True)\n",
    "                #         print(\"Common: \", len(common_elements)/len(x_tar_ids))\n",
    "\n",
    "                    ## ATTACK DATA CREATION\n",
    "\n",
    "                    ## For target model\n",
    "                #     if Yeom:\n",
    "                #         ts = x_tar_train.shape[0]/target_data_full[0].shape[0]\n",
    "                #         x_out,_,y_out,_ = train_test_split(target_data_full[0],target_data_full[1],train_size=ts,random_state=r)\n",
    "\n",
    "                #         used_labels = list(x_out.iloc[:,-1])\n",
    "                #         pred_target_in = pred_target[:x_tar_train.shape[0],:-1]\n",
    "                #         pred_target_out = np.zeros((x_out.shape[0],1))\n",
    "                #         for i in range(len(used_labels)):\n",
    "                #             pred_target_out[i] = pred_target[np.where(pred_target[:,-1] == used_labels[i]),0]\n",
    "\n",
    "                #         true_labels = np.squeeze(np.concatenate((y_tar_train.iloc[:,:-1],y_out.iloc[:,:-1])))\n",
    "                #         pred_target = np.concatenate((pred_target_in,pred_target_out))\n",
    "                #         x_tar_train_red = x_tar_train.iloc[:,:-1]\n",
    "                #         x_tar_test_red = x_out.iloc[:,:-1]\n",
    "                #     else:\n",
    "                #         true_labels = np.squeeze(np.concatenate((y_tar_train.iloc[:,:-1],y_tar_test.iloc[:,:-1])))\n",
    "                #         pred_target = pred_target[:,:-1]\n",
    "                #         x_tar_train_red = x_tar_train.iloc[:,:-1]\n",
    "                #         x_tar_test_red = x_tar_test.iloc[:,:-1]\n",
    "\n",
    "                    true_labels = np.squeeze(np.concatenate((y_tar_train.iloc[:,:-1],y_tar_test.iloc[:,:-1])))\n",
    "                #     pred_target = pred_target[:,:-1] for Yeom approach\n",
    "                #     pred_target = pred_target[:,:-1]\n",
    "                    x_tar_train_red = x_tar_train.iloc[:,:-1]\n",
    "                    x_tar_test_red = x_tar_test.iloc[:,:-1]\n",
    "\n",
    "                    target_data = np.concatenate((x_tar_train_red,x_tar_test_red))\n",
    "                    tar_lab = np.concatenate((np.ones(x_tar_train_red.shape[0]),np.zeros(x_tar_test_red.shape[0])))\n",
    "\n",
    "                    # For reference model\n",
    "\n",
    "                    '''\n",
    "                    Since DP_impurity uses impurities and impurities is also calculated for leaf nodes '-2'.\n",
    "                    '''\n",
    "                    if influence_type == 'DP_impurity' or influence_type == 'DP_threshold' or influence_type == 'DP_nodesamples':\n",
    "                        feats = x.shape[1]+1\n",
    "                    else:\n",
    "                        feats = x.shape[1]\n",
    "\n",
    "                    ref_val = np.zeros((n_models,pred_ref[0].shape[0],feats)) ## changed\n",
    "                    ref_lab = np.zeros((n_models,pred_ref[0].shape[0])) ## changed\n",
    "\n",
    "                #     if double_influence:\n",
    "                #         if influence_type_second == 'DP_impurity' or influence_type_second == 'DP_threshold' or influence_type_second == 'DP_nodesamples':\n",
    "                #             feats_second = x.shape[1]+1\n",
    "                #         else:\n",
    "                #             feats_second = x.shape[1]\n",
    "\n",
    "                #         ref_val_second = np.zeros((n_models,pred_ref[0].shape[0],feats_second)) ## changed\n",
    "\n",
    "                    for i in tqdm_notebook(range(n_models),desc=\"Generating Data\"):\n",
    "\n",
    "                #         if Yeom:\n",
    "                #             ts = x_ref_train[i].shape[0]/ref_data_full[0].shape[0]\n",
    "                #             x_out,_,y_out,_ = train_test_split(ref_data_full[0],ref_data_full[1],train_size=ts,random_state=r)\n",
    "\n",
    "                #             used_labels = list(x_out.iloc[:,-1])\n",
    "                #             pred_ref_in = pred_ref[i][:x_ref_train[i].shape[0],:-1]\n",
    "                #             pred_ref_out = np.zeros((x_out.shape[0],1))\n",
    "                #             for j in range(len(used_labels)):\n",
    "                #                 pred_ref_out[j] = pred_ref[i][np.where(pred_ref[i][:,-1] == used_labels[j]),0]\n",
    "\n",
    "                #             pred_ref_ = np.concatenate((pred_ref_in,pred_ref_out))\n",
    "                #             x_ref_train_red = x_ref_train[i].iloc[:,:-1]\n",
    "                #             x_ref_test_red = x_out.iloc[:,:-1]  ## changed\n",
    "\n",
    "                #         else:\n",
    "                #             pred_ref_ = pred_ref[i][:,:-1]\n",
    "                #             x_ref_train_red = x_ref_train[i].iloc[:,:-1]\n",
    "                #             x_ref_test_red = x_ref_test[i].iloc[:,:-1]  ## changed\n",
    "\n",
    "                #         pred_ref_ = pred_ref[i][:,:-1]\n",
    "                        pred_ref_ = pred_ref[i]\n",
    "                        x_ref_train_red = x_ref_train[i].iloc[:,:-1]\n",
    "                        x_ref_test_red = x_ref_test[i].iloc[:,:-1]  ## changed\n",
    "\n",
    "                        ref_data = np.concatenate((x_ref_train_red,x_ref_test_red))\n",
    "                        ref_lab[i] = np.concatenate((np.ones(x_ref_train_red.shape[0]),np.zeros(x_ref_test_red.shape[0])))\n",
    "\n",
    "                        attack_obj = attack_dataset(rc,target_model,ref_model[i],target_data,ref_data,pred_target,pred_ref_,influence_type)\n",
    "                        tar_val,ref_val[i] = attack_obj.get_data()\n",
    "\n",
    "                #         if double_influence:\n",
    "                #             attack_obj_second = attack_dataset(rc,target_model,ref_model[i],target_data,ref_data,pred_target,pred_ref_,influence_type_second)\n",
    "                #             tar_val_second,ref_val_second[i] = attack_obj_second.get_data()\n",
    "\n",
    "\n",
    "                    ## ATTACK\n",
    "\n",
    "                #     if double_influence:\n",
    "                #         ref_val = np.concatenate((ref_val,ref_val_second),axis=-1)\n",
    "                #         tar_val = np.concatenate((tar_val,tar_val_second),axis=-1)\n",
    "                #         ref_val = ref_val.reshape(n_models*(pred_ref[0].shape[0]),(feats+feats_second))\n",
    "                #     else:\n",
    "                #         ref_val = ref_val.reshape(n_models*(pred_ref[0].shape[0]),feats) ## changed\n",
    "\n",
    "                    ref_val = ref_val.reshape(n_models*(pred_ref[0].shape[0]),feats) ## changed\n",
    "\n",
    "                    ref_lab = ref_lab.reshape(n_models*(pred_ref[0].shape[0])) ## changed\n",
    "                    obj = attack(rnds[0],want_tune=attack_params['tuning'],tune_scoring=attack_params['scoring'])\n",
    "                    obj.fit(ref_val,ref_lab)\n",
    "                    preds = obj.predict(tar_val)\n",
    "\n",
    "                    ## METRICS RECORD\n",
    "                    ac+=accuracy_score(tar_lab,preds)\n",
    "                    ps+=precision_score(tar_lab,preds)\n",
    "                    rs+=recall_score(tar_lab,preds)\n",
    "                    fs+=f1_score(tar_lab,preds)\n",
    "\n",
    "\n",
    "                    b1,b2,b3,b4=baseline(target_model,target_data,tar_lab,true_labels)\n",
    "                    ac_b+=b1\n",
    "                    ps_b+=b2\n",
    "                    rs_b+=b3\n",
    "                    fs_b+=b4\n",
    "\n",
    "                    # to store\n",
    "                    tar_data_all_states.append(tar_val)\n",
    "                    ref_data_all_states.append(ref_val)\n",
    "                    tar_lab_all_states.append(tar_lab)\n",
    "                    ref_lab_all_states.append(ref_lab)\n",
    "                    true_labels_all_states.append(true_labels)\n",
    "\n",
    "                tar_val = np.array(tar_data_all_states)\n",
    "                tar_lab = np.array(tar_lab_all_states)\n",
    "                ref_val = np.array(ref_data_all_states)\n",
    "                ref_lab = np.array(ref_lab_all_states)\n",
    "                true_labels = np.array(true_labels_all_states)\n",
    "                to_store = (tar_val,tar_lab,ref_val,ref_lab,true_labels)\n",
    "                print(colored(\"################# RESULTS #################\\n\",'blue'))\n",
    "                print(\"Target train accuracy:\\t {a}\\nTarget test accuracy:\\t {b}\\nAttack accuracy:\\t {c}\\nAttack precision:\\t {d}\\nAttack recall:\\t\\t {e}\\nAttack f1:\\t\\t {f}\\nBaseline accuracy:\\t {g}\\nBaseline precision:\\t {h}\\nBaseline recall:\\t {i}\\nBaseline f1:\\t\\t {j}\\n\".format(a=tr_ac/n_states,b=te_ac/n_states,c=ac/n_states,d=ps/n_states,e=rs/n_states,f=fs/n_states,g=ac_b/n_states,h=ps_b/n_states,i=rs_b/n_states,j=fs_b/n_states))\n",
    "                write_into_files(to_store,run,dataset_name,influence_type,ref_tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
