{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The basics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import Sklearn model libraries\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score, precision_score, recall_score, precision_recall_curve\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# Import Utility libraries\n",
    "import shap\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from tqdm import tqdm_notebook\n",
    "import warnings\n",
    "import ipyparallel as ipp\n",
    "import time\n",
    "import torch as torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from termcolor import colored\n",
    "import os\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Pre-run necessities\n",
    "warnings.filterwarnings('ignore')\n",
    "rnds = [60, 452, 774, 802, 961, 626, 726, 211, 375, 448, 883, 684, 724, 333, 64, 646, 116, 714, 483, 73, 562, 703, 276, 394, 190, 675, 314, 604, 297, 266, 456, 845, 822, 529, 420, 605, 935, 733, 167, 603, 401, 948, 241, 734, 550, 65, 429, 470, 633, 627, 223, 713, 958, 40, 200, 641, 357, 778, 781, 498, 202, 349, 983, 125, 548, 331, 206, 556, 356, 805, 246, 626, 358, 393, 307, 792, 777, 169, 595, 279, 719, 902, 124, 197, 983, 499, 368, 864, 896, 887, 879, 224, 220, 926, 565, 173, 919, 3, 908, 941]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(run,dataset_name,influence_type,n_est):\n",
    "\n",
    "    parent_dir = '/home/amanmoha/influence_attack/results/actual/estimator_analysis'\n",
    "    directory = dataset_name+'/'+influence_type+'/'+'run'+run+'/'+'n_est-'+str(n_est)\n",
    "    path = os.path.join(parent_dir,directory)\n",
    "\n",
    "    with open(path+'/data.txt','rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "    with open(path+'/metrics.txt','rb') as f:\n",
    "            metrics = pickle.load(f)\n",
    "    f.close()\n",
    "        \n",
    "    return data, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class deep_attack():\n",
    "    \n",
    "    def __init__(self,lr,weight_decay,test_size,n_epochs,rs):\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.test_size = test_size\n",
    "        self.n_epochs = n_epochs\n",
    "        \n",
    "        self.train_accuracy = []\n",
    "        self.train_precision = []\n",
    "        self.train_recall = []\n",
    "        self.train_f1 = []\n",
    "        self.target_accuracy = []\n",
    "        self.target_recall = []\n",
    "        self.target_precision = []\n",
    "        self.target_f1 = []\n",
    "        \n",
    "        self.loss_train = np.zeros(n_epochs)\n",
    "        self.acc_train = np.zeros(n_epochs)\n",
    "        self.loss_test = np.zeros(n_epochs)\n",
    "        self.acc_test = np.zeros(n_epochs)\n",
    "        self.rs = rs\n",
    "        \n",
    "    def fit(self,x,y):\n",
    "        self.n_states = x.shape[0]\n",
    "        self.n_feat = x.shape[2]\n",
    "        self.n_hidden = self.n_feat*2\n",
    "\n",
    "        for i in range(self.n_states):\n",
    "\n",
    "            x_train, x_test, y_train, y_test = train_test_split(x[i],y[i],test_size=0.2,random_state=self.rs)\n",
    "            x_train_torch,x_test_torch,y_train_torch,y_test_torch = torch.Tensor(x_train),torch.Tensor(x_test),torch.Tensor(y_train),torch.Tensor(y_test)\n",
    "\n",
    "            # Model Definition\n",
    "            self.model = nn.Sequential(nn.Linear(self.n_feat,self.n_hidden),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Linear(self.n_hidden,2))\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = optim.Adam(self.model.parameters(),lr=self.lr,weight_decay=self.weight_decay)\n",
    "            \n",
    "            # Runs\n",
    "            for epoch in range(self.n_epochs):\n",
    "\n",
    "                # Initialize\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Train data through model\n",
    "                out_train = self.model(x_train_torch)\n",
    "                pred_prob_train = nn.functional.softmax(out_train)\n",
    "                pred_train = torch.argmax(pred_prob_train,axis=1)\n",
    "                y_train_torch = y_train_torch.long()\n",
    "                loss_train = criterion(pred_prob_train,y_train_torch)\n",
    "                \n",
    "                # Train data through model\n",
    "                out_test = self.model(x_test_torch)\n",
    "                pred_prob_test = nn.functional.softmax(out_test)\n",
    "                pred_test = torch.argmax(pred_prob_test,axis=1)\n",
    "                y_test_torch = y_test_torch.long()\n",
    "                loss_test = criterion(pred_prob_test,y_test_torch)\n",
    "                \n",
    "                # Learning\n",
    "                loss_train.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Record metrics\n",
    "                self.loss_train[epoch]+=loss_train\n",
    "                self.acc_train[epoch]+=accuracy_score(y_train_torch,pred_train)\n",
    "                self.loss_test[epoch]+=loss_test\n",
    "                self.acc_test[epoch]+=accuracy_score(y_test_torch,pred_test)\n",
    "                \n",
    "            self.train_accuracy.append(accuracy_score(y_train_torch,pred_train))\n",
    "            self.train_precision.append(precision_score(y_train_torch,pred_train))\n",
    "            self.train_recall.append(recall_score(y_train_torch,pred_train))\n",
    "            self.train_f1.append(f1_score(y_train_torch,pred_train))\n",
    "                \n",
    "    def predict(self,x,y):\n",
    "        \n",
    "        for i in range(self.n_states):\n",
    "            x_torch,y_torch = torch.Tensor(x[i]),torch.Tensor(y[i])\n",
    "            output = self.model(x_torch)\n",
    "            pred_prob = nn.functional.softmax(output)\n",
    "            pred = torch.argmax(pred_prob,axis=1)\n",
    "\n",
    "            self.target_accuracy.append(accuracy_score(y_torch,pred))\n",
    "            self.target_precision.append(precision_score(y_torch,pred))\n",
    "            self.target_recall.append(recall_score(y_torch,pred))\n",
    "            self.target_f1.append(f1_score(y_torch,pred))\n",
    "            \n",
    "    def get_results(self):\n",
    "        if len(self.target_accuracy) == 0:\n",
    "#             return self.loss_train/self.n_states,self.acc_train/self.n_states,self.loss_test/self.n_states,self.acc_test/self.n_states,np.mean(self.train_accuracy),np.mean(self.train_precision),np.mean(self.train_recall),np.mean(self.train_f1)\n",
    "            return self.loss_train/self.n_states,self.acc_train/self.n_states,self.loss_test/self.n_states,self.acc_test/self.n_states,self.train_accuracy,self.train_precision,self.train_recall,self.train_f1\n",
    "        else:\n",
    "#             return self.loss_train/self.n_states,self.acc_train/self.n_states,self.loss_test/self.n_states,self.acc_test/self.n_states,np.mean(self.train_accuracy),np.mean(self.train_precision),np.mean(self.train_recall),np.mean(self.train_f1),np.mean(self.target_accuracy),np.mean(self.target_precision),np.mean(self.target_recall),np.mean(self.target_f1)\n",
    "            return self.loss_train/self.n_states,self.acc_train/self.n_states,self.loss_test/self.n_states,self.acc_test/self.n_states,self.train_accuracy,self.train_precision,self.train_recall,self.train_f1,self.target_accuracy,self.target_precision,self.target_recall,self.target_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "136ffa383e9c40e6b30d05985bf4a9c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8b754399f2247dc897d0026e5145230",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Estimators:', max=30, style=ProgressStyle(description_width='â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_names = ['pima_diabetes']\n",
    "for dataset_name in tqdm_notebook(dataset_names):    \n",
    "    run = '5'\n",
    "#     dataset_name = 'hepatitis'\n",
    "    influence_type = 'SHAP'\n",
    "    n_ests = 30\n",
    "    params = {'learningRate':0.01,\n",
    "              'weightDecay':0.00005,\n",
    "              'testSize':0.2,\n",
    "              'nEpochs':100,\n",
    "              'randomState':rnds[0]}\n",
    "    tr_ac_est,te_ac_est,ac_est,ps_est,rs_est,fs_est = [],[],[],[],[],[]\n",
    "\n",
    "    for n_est in tqdm_notebook(range(1,n_ests+1),desc=\"Estimators:\"):\n",
    "\n",
    "        data,metrics = read_file(run,dataset_name,influence_type,n_est)\n",
    "        tar_val,tar_lab,ref_val,ref_lab,_ = data\n",
    "        tr_ac,te_ac,_,_,_,_,_,_,_,_ = metrics\n",
    "\n",
    "        attack_obj = deep_attack(lr=params['learningRate'],\n",
    "                                 weight_decay=params['weightDecay'],\n",
    "                                 test_size=params['testSize'],\n",
    "                                 n_epochs=params['nEpochs'],\n",
    "                                 rs=params['randomState'])\n",
    "        attack_obj.fit(ref_val,ref_lab)\n",
    "        attack_obj.predict(tar_val,tar_lab)\n",
    "        _,_,_,_,_,_,_,_,target_accuracy,target_precision,target_recall,target_f1 = attack_obj.get_results()\n",
    "\n",
    "        tr_ac_est.append(tr_ac)\n",
    "        te_ac_est.append(te_ac)\n",
    "        ac_est.append(target_accuracy)\n",
    "        ps_est.append(target_precision)\n",
    "        rs_est.append(target_recall)\n",
    "        fs_est.append(target_f1)\n",
    "\n",
    "    ac_est = np.array(ac_est)\n",
    "    ps_est = np.array(ps_est)\n",
    "    rs_est = np.array(rs_est)\n",
    "    fs_est = np.array(fs_est)\n",
    "    \n",
    "    with open('tempdata/'+dataset_name,'wb') as f:\n",
    "        pickle.dump((ac_est,ps_est,rs_est,fs_est,tr_ac_est,te_ac_est),f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('temp_data/'+dataset_name,'rb') as f:\n",
    "#     data = pickle.load(f)\n",
    "# f.close()\n",
    "# ac_est,ps_est,_,_ = data\n",
    "\n",
    "# print(\"########## {c}-{d} ##########\".format(c=dataset_name,d=influence_type))\n",
    "\n",
    "# gen_err = np.subtract(np.mean(tr_ac_est,axis=1),np.mean(te_ac_est,axis=1))\n",
    "# fig, ax = plt.subplots(figsize=(10,6))\n",
    "# ax.set_xlabel('Number of estimators')\n",
    "# ax.set_ylabel('Metric')\n",
    "# ax.plot(np.arange(1,n_ests+1),np.mean(ac_est,axis=1),label='Attack accuracy')\n",
    "# ax.plot(np.arange(1,n_ests+1),np.mean(ps_est,axis=1),label='Attack precision')\n",
    "# # ax.plot(np.arange(1,n_ests+1),np.mean(rs_est,axis=1),label='Attack recall')\n",
    "# # ax.plot(np.arange(1,n_ests+1),np.mean(fs_est,axis=1),label='Attack f1')\n",
    "# ax.tick_params(axis='y')\n",
    "\n",
    "# axtwin = ax.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "# axtwin.set_ylabel('Generalization Error',color='magenta')  # we already handled the x-label with ax1\n",
    "# axtwin.plot(np.arange(1,n_ests+1),gen_err,'--',label='Generalization Error',color='magenta')\n",
    "# axtwin.tick_params(axis='y',labelcolor='magenta')\n",
    "# # ax.set_ylim(0.4,0.7)\n",
    "\n",
    "# if dataset_name == 'german_credit':\n",
    "#     axtwin.set_ylim(0.07,0.14)\n",
    "# elif dataset_name == 'bcw':\n",
    "#     axtwin.set_ylim(0.03,0.07)\n",
    "# elif dataset_name == 'pima_diabetes':\n",
    "#     axtwin.set_ylim(0.06,0.11)\n",
    "\n",
    "\n",
    "# ax.legend(loc='upper left')\n",
    "# axtwin.legend(loc='upper right')\n",
    "# fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "\n",
    "# # BOX PLOTS\n",
    "\n",
    "# plt.figure(figsize=(20,12))\n",
    "# ac_list = []\n",
    "# for ac in ac_est:\n",
    "#     ac_list.append(list(ac))\n",
    "# f1 = plt.boxplot(ac_list)\n",
    "# plt.set_xlabel('Number of estimators')\n",
    "# plt.set_ylabel('Accuracy')\n",
    "\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
